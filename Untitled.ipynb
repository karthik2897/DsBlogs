{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892047e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://analytics.solenis.com/#/site/Production/views/DailyAPdashboard/Invoicesrecorded?:iid=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcdfb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "res=requests.get(url)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028360d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(res.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0df65dfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13868/937584487.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "soup.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed504893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5007573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A803377\\AppData\\Local\\Temp/ipykernel_13868/334148361.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver= webdriver.Chrome(r\"C://Driver/chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "driver= webdriver.Chrome(r\"C://Driver/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import urllib\n",
    "import sqlalchemy as sa\n",
    "import pyodbc\n",
    "import time\n",
    "from fast_to_sql import fast_to_sql as fts\n",
    "\n",
    "start = time.time()\n",
    "#Reading Data From Path\n",
    "df=pd.read_excel(r'C:\\Users\\A803377\\OneDrive - Solenis LLC\\Documents\\TREK-All-Learners.xlsx')\n",
    "df1=pd.read_excel(r'C:\\Users\\A803377\\OneDrive - Solenis LLC\\Documents\\TREK-All-Learners2.xlsx')\n",
    "df = pd.merge(df, df1,on=['Last Name','First Name','Username','Date Created','Group',\n",
    "                              'SubGroups','Coaches','Status','Coach First Name','Coach Last Name','Role','Is Coach'], how=\"left\")\n",
    "\n",
    "#Setting the Index To Required Columns \n",
    "df = df.set_index(['Last Name','First Name','Username','Date Created','Group','SubGroups','Coaches','Status','Coach First Name','Coach Last Name','Role','Is Coach'\n",
    "])\n",
    "\n",
    "#Adding 1st level name to Score,LearningPath and Status\n",
    "df.columns = ['COURSESCORE ' + col if 'Score' in col.split() else col for col in df.columns]\n",
    "df.columns = ['COURSELEARNINGPATH ' + col if 'Path' in col.split() else col for col in df.columns]\n",
    "df.columns = ['COURSESTATUS ' + col if 'Status' in col.split() else col for col in df.columns]\n",
    "\n",
    "#Creating 1st level name for unwanted columns \n",
    "df.columns = ['COURSEDrop ' + col if 'Artifacts?' in col.split() else col for col in df.columns]\n",
    "df.columns = ['COURSEDrop ' + col if 'Followup?' in col.split() else col for col in df.columns]\n",
    "df.columns = ['COURSEDrop ' + col if 'Link(s)' in col.split() else col for col in df.columns]\n",
    "\n",
    "#Creating MultiLevel Index\n",
    "df.columns = df.columns.str.partition(' ').droplevel(1)\n",
    "\n",
    "#Dropping unwanted Columns \n",
    "df= df.drop(['COURSEDrop'], axis = 1)\n",
    "\n",
    "#Creating Dataframes For Score,Learning and Status\n",
    "df_Score= df.drop(['COURSELEARNINGPATH','COURSESTATUS'], axis = 1)\n",
    "df_Learning= df.drop(['COURSESCORE','COURSESTATUS'], axis = 1)\n",
    "df_Status= df.drop(['COURSELEARNINGPATH','COURSESCORE'], axis = 1)\n",
    "\n",
    "#Stacking the Dataframes\n",
    "df_Score = df_Score.stack(1).reset_index().rename(columns={'level_12':'COURSESCORE','COURSESCORE':'SCORE_VALUE'})\n",
    "df_Learning = df_Learning.stack(1).reset_index().rename(columns={'level_12':'COURSELEARNINGPATH','COURSELEARNINGPATH':'LEARNINGPATH_VALUE'})\n",
    "df_Status = df_Status.stack(1).reset_index().rename(columns={'level_12':'COURSESTATUS','COURSESTATUS':'STATUS_Value'})\n",
    "\n",
    "#Creating Course_ID to Use it in Meargning the Dataframes \n",
    "df_Score['Course_ID'] = df_Score['COURSESCORE'].apply(lambda x: x.split(':')[0])\n",
    "df_Score['Course_ID'] = df_Score['Course_ID'].apply(lambda x: x.split('.')[0])\n",
    "df_Learning['Course_ID'] = df_Learning['COURSELEARNINGPATH'].apply(lambda x: x.split(':')[0])\n",
    "df_Learning['Course_ID'] = df_Learning['Course_ID'].apply(lambda x: x.split('.')[0])\n",
    "df_Status['Course_ID'] = df_Status['COURSESTATUS'].apply(lambda x: x.split(':')[0])\n",
    "df_Status['Course_ID'] = df_Status['Course_ID'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "# Merge List \n",
    "Mergelist=['Last Name','First Name','Username','Date Created','Group','SubGroups','Coaches','Status','Coach First Name','Coach Last Name','Role','Is Coach','Course_ID']\n",
    "#Merging the DataFrames \n",
    "df_merged = pd.merge(pd.merge(df_Score, df_Learning,on= Mergelist, how=\"left\"),df_Status,on= Mergelist,how=\"left\")\n",
    "#Update Date \n",
    "df_merged['Created_date']= pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "#Adding Blank Columns \n",
    "df_merged[\"GROUP2\"] = \"\"\n",
    "df_merged[\"RESULT\"] = \"\"\n",
    "df_merged[\"COURSE_NAME\"] = \"\"\n",
    "df_merged[\"SALESOFFICE\"] = \"\"\n",
    "#Droping Course_ID\n",
    "#df_merged=df_merged.drop(['Course_ID'], axis = 1)\n",
    "#Reindexing to match SQL Format\n",
    "df_merged = df_merged.reindex(['Last Name','First Name','Username','Group','SubGroups','Coaches','Status','Coach First Name','Coach Last Name','Role','Is Coach','COURSE_NAME','RESULT','GROUP2','Created_date','SALESOFFICE','Date Created','COURSESCORE','SCORE_VALUE','COURSESTATUS','STATUS_Value','COURSELEARNINGPATH','LEARNINGPATH_VALUE'], axis=1)\n",
    "#Changing Column Name to Match SQL FORMAT\n",
    "df_merged = df_merged.rename(columns = {\"Date Created\":\"DATECREATED\"})\n",
    "#Changing dtype to object\n",
    "df_merged['DATECREATED'] = df_merged['DATECREATED'].dt.strftime('%m-%d-%Y')\n",
    "#df_merged['DATECREATED'] = df_merged['DATECREATED'].astype(object)\n",
    "#df_merged['SCORE_VALUE'] = df_merged['SCORE_VALUE'].astype(object)\n",
    "df_test=df_merged.head(20)\n",
    "end = time.time()\n",
    "delta = end - start\n",
    "print(\"took %.2f seconds to process\" % delta)\n",
    "print('Data Retrived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "conn = pyodbc.connect(\"\"\"\n",
    "DRIVER={SQL Server Native Client 11.0};\n",
    "SERVER= NAAZUREEDWS01;\n",
    "DATABASE=TableauDB;\n",
    "trusted_connection=Yes\"\"\")\n",
    "\n",
    "fts.fast_to_sql(df_merged,'FinalTrekAssessmentData',conn, if_exists=\"append\", custom=None, temp=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "end = time.time()\n",
    "delta = end - start\n",
    "print(\"took %.2f seconds to process\" % delta)\n",
    "print(\"Update Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aab00b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a9ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c22ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e7bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e013fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\"\"\"Module for processing month-end data\"\"\"\n",
    "import calendar\n",
    "import datetime\n",
    "import glob\n",
    "import hashlib\n",
    "import logging\n",
    "import os.path\n",
    "from copy import copy\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "#import win32com.client as win32\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from apis.google import BigQuery, BigQueryEmailMeter\n",
    "from apis.sap import ZS16\n",
    "from business_data.code import SaveBusinessData, Logging, Utilities\n",
    "from business_data.logic import BusinessLogic, Locations\n",
    "from business_data.transaction_data import CreditHeaderData, OrderHeaderData, OrderLineData, CallData\n",
    "from extraction.email_meter import EmailMeter\n",
    "from reporting.daily_sap_reports import DailySAPSFReporting\n",
    "from templates.reporting import MonthEndReporting\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MonthlyReporting:\n",
    "    \"\"\"Class to run all month-end reports\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def run_month_end_reports():\n",
    "        \"\"\"Runs all month-end reports\"\"\"\n",
    "        reports_list = [\n",
    "            MonthlyOrderReport,\n",
    "            MonthlyCallReport,\n",
    "            MonthlyDispositionReport,\n",
    "            #MonthlySalesforceSAPReport,\n",
    "            #EuropeanManufacturingLogisticsReport,\n",
    "            MonthlyCSVolumesReport,\n",
    "            MonthlyOrderTypes,\n",
    "            MonthlyFive9EmailReport,\n",
    "            MonthlyEmailReport\n",
    "        ]\n",
    "        for monthly_report in reports_list:\n",
    "            try:\n",
    "                monthly_report().create_all_monthly_files()\n",
    "            except Exception as e:\n",
    "                log.exception(e)\n",
    "                continue\n",
    "\n",
    "\n",
    "class MonthEndSharedData:\n",
    "    \"\"\"Class to hold shared data for month-end reporting\"\"\"\n",
    "\n",
    "    def __init__(self, year: int = None, month: int = None):\n",
    "        super().__init__()\n",
    "        if year is None or month is None:\n",
    "            report_date = datetime.date.today().replace(day=1) - datetime.timedelta(days=1)\n",
    "            self.year = report_date.year\n",
    "            self.month = report_date.month\n",
    "        else:\n",
    "            self.month = month\n",
    "            self.year = year\n",
    "\n",
    "        self.year_month = f'{str(self.year)}-{str(self.month).zfill(2)}'\n",
    "\n",
    "        self.report_prefix = f'{self.year_month} - '\n",
    "        self.report_name = ''\n",
    "\n",
    "        self.save_location = os.path.join(Locations.kpi_root_dir, 'Monthly', str(self.year), self.year_month)\n",
    "        self.qv_file_location = os.path.join(self.save_location, 'QlikView')\n",
    "\n",
    "        prior_report_date = datetime.date(self.year, self.month, 1) - datetime.timedelta(days=1)\n",
    "        prior_report_year_month = f'{str(prior_report_date.year)}-{str(prior_report_date.month).zfill(2)}'\n",
    "        self.prior_save_location = os.path.join(Locations.kpi_root_dir, 'Monthly', str(prior_report_date.year),\n",
    "                                                prior_report_year_month)\n",
    "        self.prior_report_prefix = f'{prior_report_year_month} - '\n",
    "        self.prior_report_name = ''\n",
    "\n",
    "        for directory in [self.save_location, self.qv_file_location]:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "    def __del__(self) -> None:\n",
    "        if Locations.environment != 'dev':\n",
    "            pattern = os.path.join(Locations.temp_dir, self.report_prefix)\n",
    "            for temp_file in glob.glob(f'{pattern}*'):\n",
    "                log.warning(f'Deleting temp file {temp_file}')\n",
    "                os.remove(temp_file)\n",
    "        else:\n",
    "            log.info('Not deleting temporary files, as on dev machine')\n",
    "\n",
    "    @staticmethod\n",
    "    def __add_location_details__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.merge(BusinessLogic.cs_location_hierarchy().loc[:, ['country', 'location', 'region']])\n",
    "\n",
    "    def __add_reporting_month__(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(reporting_month=lambda x: f'01/{self.month}/{self.year}')\n",
    "\n",
    "\n",
    "class MonthlyOrderReport(MonthEndSharedData, MonthEndReporting):\n",
    "    \"\"\"Creates the monthly order header and lines reports\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.report_name = f'{self.report_prefix}Month-End Orders'\n",
    "\n",
    "        self.header_data = None\n",
    "        self.line_data = None\n",
    "        self.combined_data = None\n",
    "        self.location_output_columns = ['Reporting Month', 'Location', 'Country', 'Division', 'Auto Orders',\n",
    "                                        'Manual Orders', 'Conexiom Orders', 'eShop Orders', 'EDI Orders',\n",
    "                                        'Sales Force Orders', 'Cloud for Customer Orders', '# Orders Placed',\n",
    "                                        'Auto Lines', 'Manual Lines', 'Conexiom Lines', 'eShop Lines', 'EDI Lines',\n",
    "                                        'Sales Force Lines', 'Cloud for Customer Lines', '# Order Lines Placed']\n",
    "        self.csr_output_columns = ['Reporting Month', 'Created', 'Location', 'Country', 'Division', 'Auto Orders',\n",
    "                                   'Manual Orders', 'Conexiom Orders', 'eShop Orders', 'EDI Orders',\n",
    "                                   'Sales Force Orders', 'Cloud for Customer Orders', '# Orders Placed', 'Auto Lines',\n",
    "                                   'Manual Lines', 'Conexiom Lines', 'eShop Lines', 'EDI Lines', 'Sales Force Lines',\n",
    "                                   'Cloud for Customer Lines', '# Order Lines Placed']\n",
    "\n",
    "    def create_all_monthly_files(self) -> None:\n",
    "        \"\"\"Creates monthly credit file in main KPI folder\"\"\"\n",
    "        self.process_monthly_data()\n",
    "        self.create_monthly_data_files()\n",
    "        self.create_monthly_qv_files()\n",
    "        self.update_bigquery()\n",
    "        self.create_bigquery_view()\n",
    "\n",
    "    def process_monthly_data(self) -> None:\n",
    "        \"\"\"Processes the monthly data ready for outputs to be created\"\"\"\n",
    "        log.info(f'Processing {self.report_name}')\n",
    "        self._get_header_data_()\n",
    "        self._get_line_data_()\n",
    "        self._merge_header_and_line_data_()\n",
    "\n",
    "    def create_monthly_data_files(self) -> None:\n",
    "        \"\"\"Creates monthly QlikView files\"\"\"\n",
    "        log.info(f'Creating data files for {self.report_name}')\n",
    "\n",
    "        header_file = os.path.join(self.save_location, f'{self.report_prefix}Global Order Headers.csv')\n",
    "        self.combined_data.to_csv(header_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        line_file = os.path.join(self.save_location, f'{self.report_prefix}Global Order Lines.csv')\n",
    "        self.line_data.to_csv(line_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    def create_monthly_qv_files(self) -> None:\n",
    "        \"\"\"Creates monthly data files\"\"\"\n",
    "        log.info(f'Creating QlikView files for {self.report_name}')\n",
    "\n",
    "        self.__create_location_file__(self.combined_data)\n",
    "        self.__create_csr_file__(self.combined_data)\n",
    "\n",
    "    def update_bigquery(self) -> None:\n",
    "        \"\"\"Updates the month-end BigQuery order_data table with the monthly data\"\"\"\n",
    "        log.info(f'Updating BigQuery table order_data_all')\n",
    "\n",
    "        bq = BigQuery(dataset='month_end_kpi_trends_feedback',\n",
    "                      table='order_data_all')\n",
    "\n",
    "        order_data = self.combined_data.loc[:, ['created_on', 'lines', 'country', 'division', 'sales_doc', 'system']] \\\n",
    "            .rename(columns={'lines': 'number_of_lines'}) \\\n",
    "            .fillna(0.0) \\\n",
    "            .astype({'created_on': 'datetime64[s]',\n",
    "                     'number_of_lines': int,\n",
    "                     'country': str,\n",
    "                     'division': str,\n",
    "                     'sales_doc': str,\n",
    "                     'system': str})\n",
    "\n",
    "        order_data.division = order_data.division.map({'Diversey Care': 'Prof',\n",
    "                                                       'Hygiene Solutions': 'F&B'})\n",
    "\n",
    "        order_data['id_data'] = order_data.created_on.astype(str) \\\n",
    "                                + order_data.country \\\n",
    "                                + order_data.division \\\n",
    "                                + order_data.sales_doc \\\n",
    "                                + order_data.system\n",
    "        order_data['id'] = order_data.apply(lambda x: hashlib.sha256(x.id_data.encode()).hexdigest(), axis=1)\n",
    "        order_data.drop(columns=['id_data'], inplace=True)\n",
    "        order_data['processed_time'] = datetime.datetime.now(pytz.UTC)\n",
    "        order_data['exclude_order'] = False\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition='WRITE_APPEND')\n",
    "\n",
    "        job = bq.bigquery.load_table_from_dataframe(order_data,\n",
    "                                                    destination=bq.destination_table,\n",
    "                                                    job_config=job_config)\n",
    "        job.result()\n",
    "\n",
    "    @staticmethod\n",
    "    def create_bigquery_view():\n",
    "        \"\"\"Creates a View in BigQuery to only show the most recently updated call data\"\"\"\n",
    "        log.info(f'Creating BigQuery view on order_data_all')\n",
    "\n",
    "        bq = BigQuery()\n",
    "\n",
    "        view_id = 'global-cs-monthly-kpis.month_end_kpi_trends_feedback.order_data'\n",
    "        view = bigquery.Table(view_id)\n",
    "        view.view_query = f\"\"\"\n",
    "            SELECT\n",
    "                agg.table.*\n",
    "            FROM (\n",
    "                SELECT\n",
    "                id,\n",
    "                ARRAY_AGG(STRUCT(table)\n",
    "                ORDER BY\n",
    "                    processed_time DESC)[SAFE_OFFSET(0)] agg\n",
    "            FROM\n",
    "                `month_end_kpi_trends_feedback.order_data_all` table\n",
    "            GROUP BY\n",
    "                id)\n",
    "            \"\"\"\n",
    "\n",
    "        bq.bigquery.delete_table(bigquery.Table(view_id))\n",
    "        bq.bigquery.create_table(view)\n",
    "\n",
    "    def _get_header_data_(self) -> None:\n",
    "        monthly_orders = self.__load_headers__()\n",
    "        monthly_orders = self.__fix_country_names__(monthly_orders)\n",
    "        monthly_orders = self.__filter_non_cs_orders__(monthly_orders)\n",
    "        monthly_orders = self.__fix_dtypes__(monthly_orders)\n",
    "        monthly_orders = self.__fix_divisions__(monthly_orders)\n",
    "        monthly_orders = self.__add_cs_locations__(monthly_orders)\n",
    "        monthly_orders = self.__add_order_types__(monthly_orders)\n",
    "        monthly_orders = self.__add_reporting_month__(monthly_orders)\n",
    "\n",
    "        self.header_data = monthly_orders\n",
    "\n",
    "    def _get_line_data_(self) -> None:\n",
    "        self.line_data = self.__load_order_lines__()\n",
    "        self.line_count = self.__create_pivot__(self.line_data)\n",
    "\n",
    "    def _merge_header_and_line_data_(self) -> None:\n",
    "        self.combined_data = self.header_data.assign(orders=1) \\\n",
    "            .merge(self.line_count, how='left', left_on='sales_doc', right_index=True)\n",
    "\n",
    "    def __load_headers__(self) -> pd.DataFrame:\n",
    "        return OrderHeaderData(name=f'{self.report_name} - Headers').get_monthly_data(self.year, self.month)\n",
    "\n",
    "    @staticmethod\n",
    "    def __fix_country_names__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.replace({'United Kingdom': 'United Kingdom (Indirect)',\n",
    "                             'United Kingdom (Zenith)': 'United Kingdom (Direct)'})\n",
    "\n",
    "    @staticmethod\n",
    "    def __filter_non_cs_orders__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        cs_only_data = data.merge(BusinessLogic.cs_employees_and_system_users()) \\\n",
    "            .dropna(subset=['created_by'])\n",
    "        return cs_only_data\n",
    "\n",
    "    @staticmethod\n",
    "    def __fix_dtypes__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.astype({'country': str, 'division': str})\n",
    "\n",
    "    @staticmethod\n",
    "    def __fix_divisions__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        data.division = data.division.map({'Professional': 'Diversey Care',\n",
    "                                           'F&B': 'Hygiene Solutions'})\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def __add_cs_locations__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.merge(BusinessLogic.cs_location_hierarchy().loc[:, ['country', 'location']])\n",
    "\n",
    "    @staticmethod\n",
    "    def __add_order_types__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        data['type'] = data.apply(Utilities().get_order_type, axis='columns')\n",
    "        return data\n",
    "\n",
    "    def __load_order_lines__(self) -> pd.DataFrame:\n",
    "        return OrderLineData(name=f'{self.report_name} - Lines').get_monthly_data(self.year, self.month)\n",
    "\n",
    "    @staticmethod\n",
    "    def __create_pivot__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(lines=1) \\\n",
    "            .pivot_table(index='sales_doc', values='lines', aggfunc=np.sum)\n",
    "\n",
    "    def __create_location_file__(self, data):\n",
    "        location_data = self.__get_qv_pivot(data, ['reporting_month', 'location', 'country', 'division'])\n",
    "        file_name = os.path.join(self.qv_file_location, f'{self.report_prefix}Location Volumes.csv')\n",
    "\n",
    "        self.__create_qv_file__(location_data, self.location_output_columns, file_name)\n",
    "\n",
    "    def __create_csr_file__(self, data):\n",
    "        csr_data = self.__get_qv_pivot(data, ['reporting_month', 'created_by', 'location', 'country', 'division'])\n",
    "        file_name = os.path.join(self.qv_file_location, f'{self.report_prefix}CSR Volumes.csv')\n",
    "\n",
    "        self.__create_qv_file__(csr_data, self.csr_output_columns, file_name)\n",
    "\n",
    "    def __create_qv_file__(self, pivot: pd.DataFrame, output_columns: list, file_name: str):\n",
    "        qv_data = self.__rename_columns__(pivot)\n",
    "        qv_data = self.__add_missing_columns__(qv_data, output_columns)\n",
    "        qv_data = self.__fix_column_order__(qv_data, output_columns)\n",
    "\n",
    "        self.__save_qv_file__(qv_data, file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_qv_pivot(data: pd.DataFrame, index_cols: list) -> pd.DataFrame:\n",
    "        qv_data = data.pivot_table(index=index_cols,\n",
    "                                   columns=['type'],\n",
    "                                   values=['orders', 'lines'],\n",
    "                                   aggfunc=np.sum,\n",
    "                                   fill_value=0,\n",
    "                                   margins_name='Total',\n",
    "                                   margins=True) \\\n",
    "            .reset_index()\n",
    "\n",
    "        qv_data.columns = [' '.join(col).strip() for col in qv_data.columns]\n",
    "        qv_data = qv_data.drop(qv_data.tail(1).index)\n",
    "        return qv_data\n",
    "\n",
    "    @staticmethod\n",
    "    def __rename_columns__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.rename(columns={'reporting_month': 'Reporting Month',\n",
    "                                    'created_by': 'Created',\n",
    "                                    'location': 'Location',\n",
    "                                    'country': 'Country',\n",
    "                                    'division': 'Division',\n",
    "                                    'orders Auto Loader': 'Auto Orders',\n",
    "                                    'orders Manual': 'Manual Orders',\n",
    "                                    'orders Conexiom': 'Conexiom Orders',\n",
    "                                    'orders eShop': 'eShop Orders',\n",
    "                                    'orders EDI': 'EDI Orders',\n",
    "                                    'orders SalesForce': 'Sales Force Orders',\n",
    "                                    'orders Total': '# Orders Placed',\n",
    "                                    'lines Auto Loader': 'Auto Lines',\n",
    "                                    'lines Manual': 'Manual Lines',\n",
    "                                    'lines Conexiom': 'Conexiom Lines',\n",
    "                                    'lines eShop': 'eShop Lines',\n",
    "                                    'lines EDI': 'EDI Lines',\n",
    "                                    'lines SalesForce': 'Sales Force Lines',\n",
    "                                    'lines Total': '# Order Lines Placed',\n",
    "                                    })\n",
    "\n",
    "    @staticmethod\n",
    "    def __add_missing_columns__(data: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "        for column in columns:\n",
    "            if column not in data:\n",
    "                data[column] = 0\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def __fix_column_order__(data: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "        return data.loc[:, columns]\n",
    "\n",
    "    @staticmethod\n",
    "    def __save_qv_file__(data: pd.DataFrame, file_name: str) -> None:\n",
    "        data.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "class MonthlyCallReport(MonthEndSharedData, MonthEndReporting):\n",
    "    \"\"\"Creates the monthly call report and QlikView file\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.report_name = f'{self.report_prefix}Month-End Calls'\n",
    "\n",
    "        self.call_data = pd.DataFrame()\n",
    "        self.output_columns = ['date', 'time', 'campaign', 'skill', 'disposition', 'queue_time', 'called_party',\n",
    "                               'result', 'country', 'location', 'region']\n",
    "        self.answered_results = ['Answered', 'Transfer']\n",
    "        self.abandoned_results = ['Disconnected', 'Abandon', 'Queue Timeout', 'ExitEmpty', 'ExitWithTimeout', 'OptOut']\n",
    "\n",
    "    def create_all_monthly_files(self) -> None:\n",
    "        \"\"\"Creates monthly data, and QlikView import files\"\"\"\n",
    "        self.process_monthly_data()\n",
    "        self.create_monthly_data_files()\n",
    "        self.create_monthly_qv_files()\n",
    "        self.update_bigquery()\n",
    "        self.create_bigquery_view()\n",
    "\n",
    "    def process_monthly_data(self) -> None:\n",
    "        \"\"\"Processes the monthly data ready for outputs to be created\"\"\"\n",
    "        log.info(f'Processing {self.report_name}')\n",
    "\n",
    "        call_data = CallData(name=self.report_name).get_monthly_data(self.year, self.month)\n",
    "        call_data = call_data.drop(columns={'country'})\n",
    "        self.call_data = call_data.merge(BusinessLogic.call_queue_to_country(),\n",
    "                                         left_on='called_party', right_on='queue')\n",
    "\n",
    "    def create_monthly_data_files(self) -> None:\n",
    "        \"\"\"Creates monthly data files\"\"\"\n",
    "        log.info(f'Creating data files for {self.report_name}')\n",
    "\n",
    "        monthly_data = self.__add_date__(self.call_data)\n",
    "        monthly_data = self.__add_time__(monthly_data)\n",
    "        monthly_data = self.__add_location_details__(monthly_data)\n",
    "        monthly_data = self.__set_column_order__(monthly_data)\n",
    "\n",
    "        file_name = os.path.join(self.save_location, f'{self.report_prefix}Global CS Calls.csv')\n",
    "        monthly_data.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    def create_monthly_qv_files(self) -> None:\n",
    "        \"\"\"Creates monthly QlikView file\"\"\"\n",
    "        log.info(f'Creating QlikView files for {self.report_name}')\n",
    "\n",
    "        qv_data = self.__add_reporting_month__(self.call_data)\n",
    "        qv_data = self.__add_calls__(qv_data)\n",
    "        qv_data = self.__add_abandoned__(qv_data)\n",
    "        qv_data = self.__add_answered__(qv_data)\n",
    "        qv_data = self.__add_service_level__(qv_data)\n",
    "        qv_data = self.__add_location_details__(qv_data)\n",
    "        qv_data = self.__pivot_data__(qv_data)\n",
    "        qv_data = self.__rename_columnes__(qv_data)\n",
    "\n",
    "        filename = os.path.join(self.qv_file_location, f'{self.report_prefix}Global Calls.csv')\n",
    "        qv_data.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    def update_bigquery(self) -> None:\n",
    "        \"\"\"Updates the month-end BigQuery call_data table with the monthly data\"\"\"\n",
    "        log.info(f'Updating BigQuery data for {self.report_name}')\n",
    "\n",
    "        call_data = self.call_data.loc[:, ['call_time', 'country', 'result', 'disposition', 'queue_time']] \\\n",
    "            .query(\"country != 'Non-CS Call'\") \\\n",
    "            .astype({'call_time': 'datetime64[s]',\n",
    "                     'queue_time': int,\n",
    "                     'country': str,\n",
    "                     'result': str,\n",
    "                     'disposition': str}) \\\n",
    "            .replace(to_replace={'United Kingdom': 'United Kingdom (Indirect)',\n",
    "                                 'United Kingdom (Zenith)': 'United Kingdom (Direct)'})\n",
    "\n",
    "        call_data['id_data'] = call_data.call_time.astype(str) \\\n",
    "                               + call_data.country \\\n",
    "                               + call_data.result \\\n",
    "                               + call_data.disposition \\\n",
    "                               + call_data.queue_time.astype(str)\n",
    "        call_data['id'] = call_data.apply(lambda x: hashlib.sha256(x.id_data.encode()).hexdigest(), axis=1)\n",
    "        call_data.drop(columns=['id_data'], inplace=True)\n",
    "        call_data['processed_time'] = datetime.datetime.now(pytz.UTC)\n",
    "        call_data['exclude_call'] = False\n",
    "\n",
    "        bq = BigQuery(dataset='month_end_kpi_trends_feedback',\n",
    "                      table='call_data_all')\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition='WRITE_APPEND')\n",
    "        bq.bigquery.load_table_from_dataframe(call_data,\n",
    "                                              destination=bq.destination_table,\n",
    "                                              job_config=job_config) \\\n",
    "            .result()\n",
    "\n",
    "    @staticmethod\n",
    "    def create_bigquery_view():\n",
    "        \"\"\"Creates a View in BigQuery to only show the most recently updated call data\"\"\"\n",
    "        log.info(f'Creating BigQuery view on call_data_all')\n",
    "\n",
    "        bq = BigQuery()\n",
    "\n",
    "        view_id = 'global-cs-monthly-kpis.month_end_kpi_trends_feedback.call_data'\n",
    "        view = bigquery.Table(view_id)\n",
    "        view.view_query = f\"\"\"\n",
    "            SELECT\n",
    "                agg.table.*\n",
    "            FROM (\n",
    "                SELECT\n",
    "                id,\n",
    "                ARRAY_AGG(STRUCT(table)\n",
    "                ORDER BY\n",
    "                    processed_time DESC)[SAFE_OFFSET(0)] agg\n",
    "            FROM\n",
    "                `month_end_kpi_trends_feedback.call_data_all` table\n",
    "            GROUP BY\n",
    "                id)\n",
    "            \"\"\"\n",
    "\n",
    "        bq.bigquery.delete_table(bigquery.Table(view_id))\n",
    "        bq.bigquery.create_table(view)\n",
    "\n",
    "    @staticmethod\n",
    "    def __add_date__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(date=lambda x: x.call_time.dt.date)\n",
    "\n",
    "    @staticmethod\n",
    "    def __add_time__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(time=lambda x: x.call_time.dt.time)\n",
    "\n",
    "    def __set_column_order__(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.loc[:, self.output_columns]\n",
    "\n",
    "    def __add_reporting_month__(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        reporting_month = datetime.date(self.year, self.month, 1).strftime('%d/%m/%Y')\n",
    "        return data.assign(reporting_month=reporting_month)\n",
    "\n",
    "    @staticmethod\n",
    "    def __add_calls__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(calls=1)\n",
    "\n",
    "    def __add_abandoned__(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(abandoned=lambda x: np.where(x.result.isin(self.abandoned_results), 1, 0))\n",
    "\n",
    "    def __add_answered__(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(answered=lambda x: np.where(x.result.isin(self.answered_results), 1, 0))\n",
    "\n",
    "    @staticmethod\n",
    "    def __add_service_level__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(sl_call=lambda x: np.where(np.logical_and(x.answered == 1, x.queue_time <= 20), 1, 0))\n",
    "\n",
    "    @staticmethod\n",
    "    def __pivot_data__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.pivot_table(index=['reporting_month', 'location', 'country'],\n",
    "                                values=['abandoned', 'answered', 'sl_call', 'calls'],\n",
    "                                aggfunc=np.sum\n",
    "                                ).reset_index()\n",
    "\n",
    "    @staticmethod\n",
    "    def __rename_columnes__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.rename(columns={'reporting_month': 'Reporting Month',\n",
    "                                    'location': 'Location',\n",
    "                                    'country': 'Country',\n",
    "                                    'abandoned': 'Abandoned Calls',\n",
    "                                    'answered': 'Answered Calls',\n",
    "                                    'sl_call': 'SL Calls',\n",
    "                                    'calls': 'Total Calls'})\n",
    "\n",
    "\n",
    "class MonthlyDispositionReport(MonthEndSharedData, MonthEndReporting):\n",
    "    \"\"\"Class to create the monthly Five9 contact disposition report files\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.report_name = f'{self.report_prefix}Month-End Dispositions'\n",
    "\n",
    "        self.call_data = pd.DataFrame()\n",
    "        self.disposition_data = pd.DataFrame()\n",
    "        self.no_disposition = ['No Disposition', 'Abandon', 'Z.ACW Exceeded', '', np.nan]\n",
    "\n",
    "    def create_all_monthly_files(self) -> None:\n",
    "        \"\"\"Creates monthly files for Five9 disposition reporting\"\"\"\n",
    "        self.process_monthly_data()\n",
    "        self.create_monthly_data_files()\n",
    "        self.create_monthly_qv_files()\n",
    "\n",
    "    def process_monthly_data(self) -> None:\n",
    "        \"\"\"Processes the monthly data ready for outputs to be created\"\"\"\n",
    "        log.info(f'Processing {self.report_name}')\n",
    "\n",
    "        self._load_call_data_()\n",
    "        disposition_data = self._add_answered_(self.call_data)\n",
    "        disposition_data = self.__add_location_details__(disposition_data)\n",
    "        disposition_data = self._add_dispositioned_(disposition_data)\n",
    "\n",
    "        self.disposition_data = disposition_data\n",
    "\n",
    "    def create_monthly_data_files(self) -> None:\n",
    "        \"\"\"Creates monthly data files\"\"\"\n",
    "        log.info(f'Creating data files for {self.report_name}')\n",
    "\n",
    "        file_name = os.path.join(self.save_location, f'{self.report_prefix}CS Dispositions.csv')\n",
    "        self.disposition_data.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    def create_monthly_qv_files(self) -> None:\n",
    "        \"\"\"Creates monthly QlikView file\"\"\"\n",
    "        log.info(f'Creating QlikView files for {self.report_name}')\n",
    "\n",
    "        qv_data = self.__add_reporting_month__(self.disposition_data)\n",
    "        qv_data = self._add_calls_(qv_data)\n",
    "        qv_data = self._remove_unanswered_(qv_data)\n",
    "        qv_data = self._remove_fuze_calls_(qv_data)\n",
    "        qv_data = self._create_pivot_(qv_data)\n",
    "        qv_data = self._rename_columns_(qv_data)\n",
    "\n",
    "        file_name = os.path.join(self.qv_file_location, f'{self.report_prefix}Dispositions.csv')\n",
    "        qv_data.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    def _load_call_data_(self) -> None:\n",
    "        call_data = CallData(name=self.report_name).get_monthly_data(self.year, self.month)\n",
    "        call_data = call_data.drop(columns={'country'})\n",
    "        self.call_data = call_data.merge(BusinessLogic.call_queue_to_country(),\n",
    "                                         left_on='called_party', right_on='queue')\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_answered_(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(answered=lambda x: np.where(x.result == 'Answered', 1, 0))\n",
    "\n",
    "    def _add_dispositioned_(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(dispositioned=lambda x: np.where(x.disposition.isin(self.no_disposition), 0, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_calls_(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.assign(calls=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_unanswered_(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.query(\"answered == 1\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_fuze_calls_(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.query(\"location not in ['Athens', 'Wadeville', 'Makati', 'Campinas', 'Hamilton']\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_pivot_(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.pivot_table(index=['reporting_month', 'location', 'country'],\n",
    "                                values=['calls', 'dispositioned'],\n",
    "                                aggfunc=np.sum).reset_index()\n",
    "\n",
    "    @staticmethod\n",
    "    def _rename_columns_(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.rename(columns={'reporting_month': 'Reporting Month',\n",
    "                                    'calls': '# Calls',\n",
    "                                    'dispositioned': '# Dispositioned'})\n",
    "\n",
    "\n",
    "class MonthlySalesforceSAPReport(MonthEndSharedData, DailySAPSFReporting):\n",
    "    \"\"\"Creates the month-end report to match SAP credit codes to Salesforce Case codes\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.report_name = f'{self.report_prefix}SAP-Salesforce Credits'\n",
    "        self.prior_report_name = f'{self.prior_report_prefix}SAP-Salesforce Credits'\n",
    "\n",
    "        self.data_location = os.path.join(\n",
    "            Locations.analytics_dir,\n",
    "            'sf_sap_credit_reasons'\n",
    "        )\n",
    "\n",
    "        self.credit_and_return_requests = pd.DataFrame()\n",
    "        self.salesforce_data = pd.DataFrame()\n",
    "\n",
    "        self.xl = None\n",
    "        self.output_file = None\n",
    "        self.output_file_sf_data_sheet = None\n",
    "        self.output_file_sap_data_sheet = None\n",
    "\n",
    "        self.sap_data_file = None\n",
    "        self.sap_data_file_data_sheet = None\n",
    "\n",
    "        self.sf_data_file = None\n",
    "        self.sf_data_file_data_sheet = None\n",
    "\n",
    "    def create_all_monthly_files(self):\n",
    "        \"\"\"Creates monthly outputs for the Salesforce/SAP credit alignment work\"\"\"\n",
    "        log.info(f'Running {self.report_name}')\n",
    "\n",
    "        self.process_monthly_data()\n",
    "        self.create_monthly_data_files()\n",
    "\n",
    "    def process_monthly_data(self):\n",
    "        \"\"\"Loads and processes monthly data for report\"\"\"\n",
    "        log.info(f'Processing Data')\n",
    "\n",
    "        self._get_sap_data_()\n",
    "        self._get_salesforce_data_()\n",
    "        self._save_data_files()\n",
    "\n",
    "    def create_monthly_data_files(self):\n",
    "        \"\"\"Creates output files for report\"\"\"\n",
    "        log.info(f'Creating monthly files')\n",
    "\n",
    "        self._start_excel_()\n",
    "        self._copy_prior_month_file_()\n",
    "        self._open_output_file()\n",
    "        self._open_sap_data_file_()\n",
    "        self._open_sf_data_file_()\n",
    "        self._copy_data_into_output_file_()\n",
    "        self._close_data_files()\n",
    "        self._copy_formulas_down_()\n",
    "        self._update_named_ranges_()\n",
    "        self._update_pivots_()\n",
    "        self._save_and_close_output_file_()\n",
    "        self._quit_excel_()\n",
    "\n",
    "    def _get_sap_data_(self):\n",
    "        log.info(f'Getting SAP data')\n",
    "\n",
    "        credit_return_requests = self.__load_credit_return_request_data__()\n",
    "        credit_return_requests = self.__add_location_details__(credit_return_requests)\n",
    "        credit_return_requests = self.__filter_for_li3_locations__(credit_return_requests)\n",
    "        credit_return_requests = self.__add_csr_names__(credit_return_requests)\n",
    "        credit_return_requests = self.__normalise_sf_code_data__(credit_return_requests)\n",
    "        credit_return_requests = self.__apply_business_rules__(credit_return_requests)\n",
    "        credit_return_requests = self.__filter_for_credit_or_return_requests__(credit_return_requests)\n",
    "        credit_return_requests = self.__clear_nan_values__(credit_return_requests)\n",
    "        credit_return_requests = self.__create_date_column__(credit_return_requests)\n",
    "        credit_return_requests = self.__get_required_columns__(credit_return_requests)\n",
    "        self.credit_and_return_requests = credit_return_requests\n",
    "\n",
    "    def _get_salesforce_data_(self):\n",
    "        log.info(f'Getting Salesforce data')\n",
    "\n",
    "        salesforce_data = self.__load_salesforce_data__()\n",
    "        salesforce_data = self.__set_sf_column_order__(salesforce_data)\n",
    "\n",
    "        self.salesforce_data = salesforce_data\n",
    "\n",
    "    def _save_data_files(self):\n",
    "        log.info(f'Saving monthly data files')\n",
    "\n",
    "        self.salesforce_data.to_csv(os.path.join(Locations.temp_dir, self.report_prefix + 'sf_data.csv'),\n",
    "                                    encoding='utf-8-sig', index=False)\n",
    "        self.credit_and_return_requests.to_csv(os.path.join(Locations.temp_dir, self.report_prefix + 'sap_data.csv'),\n",
    "                                               encoding='utf-8-sig', index=False)\n",
    "\n",
    "    def _start_excel_(self):\n",
    "        log.info(f'Starting Excel')\n",
    "\n",
    "        self.xl = win32.Dispatch(\"Excel.Application\")\n",
    "        if Locations.environment == \"dev\":\n",
    "            self.xl.DisplayAlerts = True\n",
    "            self.xl.Visible = True\n",
    "        else:\n",
    "            self.xl.DisplayAlerts = False\n",
    "            self.xl.Visible = False\n",
    "\n",
    "    def _copy_prior_month_file_(self):\n",
    "        log.info(f'Copying prior month file')\n",
    "\n",
    "        previous_file_path = os.path.join(self.prior_save_location, f'{self.prior_report_name}.xlsx')\n",
    "        current_file_path = os.path.join(self.save_location, f'{self.report_name}.xlsx')\n",
    "        copyfile(previous_file_path, current_file_path)\n",
    "\n",
    "    def _open_output_file(self):\n",
    "        log.info(f'Opening output file')\n",
    "\n",
    "        self.output_file = self.xl.Workbooks.Open(os.path.join(self.save_location, f'{self.report_name}.xlsx'))\n",
    "        self.output_file_sf_data_sheet = self.output_file.Sheets('SF Data')\n",
    "        self.output_file_sap_data_sheet = self.output_file.Sheets('SAP Data')\n",
    "        self.xl.Application.Calculation = -4135  # to set xlCalculationManual\n",
    "\n",
    "    def _open_sap_data_file_(self):\n",
    "        log.info(f'Opening SAP data file')\n",
    "\n",
    "        self.sap_data_file = self.xl.Workbooks. \\\n",
    "            Open(os.path.join(os.path.join(Locations.temp_dir, self.report_prefix + 'sap_data.csv')))\n",
    "        self.sap_data_file_data_sheet = self.sap_data_file.Sheets(f'{self.report_prefix}sap_data')\n",
    "\n",
    "    def _open_sf_data_file_(self):\n",
    "        log.info(f'Opening SF data file')\n",
    "\n",
    "        self.sf_data_file = self.xl.Workbooks. \\\n",
    "            Open(os.path.join(Locations.temp_dir, self.report_prefix + 'sf_data.csv'))\n",
    "        self.sf_data_file_data_sheet = self.sf_data_file.Sheets(f'{self.report_prefix}sf_data')\n",
    "\n",
    "    def _copy_data_into_output_file_(self):\n",
    "        log.info(f'Adding SAP and SF data to output file')\n",
    "\n",
    "        self.__copy_sf_data__()\n",
    "        self.__copy_sap_data__()\n",
    "\n",
    "    def _close_data_files(self):\n",
    "        log.info(f'Closing data files')\n",
    "\n",
    "        self.sap_data_file.Close(False)\n",
    "        self.sf_data_file.Close(False)\n",
    "\n",
    "    def _copy_formulas_down_(self):\n",
    "        log.info('Copying formulas down')\n",
    "\n",
    "        self.__copy_sf_formulas_down__()\n",
    "        self.__copy_sap_formulas_down__()\n",
    "\n",
    "    def _update_named_ranges_(self):\n",
    "        log.debug('Updating Named Ranges')\n",
    "\n",
    "        self.__update_sf_named_range__()\n",
    "        self.__update_sap_named_range__()\n",
    "\n",
    "    def _update_pivots_(self):\n",
    "        log.debug('Updating pivots')\n",
    "\n",
    "        self.xl.Application.Calculation = -4105  # to set xlCalculationAutomatic\n",
    "        self.output_file.RefreshAll()\n",
    "\n",
    "    def _save_and_close_output_file_(self):\n",
    "        log.debug('Saving and closing files')\n",
    "\n",
    "        self.output_file.Sheets(\"SAP Matches SF Code\").Select()\n",
    "        self.output_file.Close(True)\n",
    "\n",
    "    def _quit_excel_(self):\n",
    "        log.debug('Closing Excel')\n",
    "\n",
    "        self.xl.Quit()\n",
    "        del self.xl\n",
    "\n",
    "    def __load_credit_return_request_data__(self) -> pd.DataFrame:\n",
    "        log.debug(f'Loading credit and return data')\n",
    "\n",
    "        return OrderHeaderData().get_monthly_data(self.year, self.month)\n",
    "\n",
    "    @staticmethod\n",
    "    def __filter_for_li3_locations__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        log.debug(f'Filtering LI3 locations')\n",
    "\n",
    "        return data.query(\"location in ['Athens', 'Chartres', 'Monza', 'Vedbk', \"\n",
    "                          \"'Utrecht', 'Viladecans', 'Warsaw', 'Warsaw - DACH']\")\n",
    "\n",
    "    @staticmethod\n",
    "    def __filter_for_credit_or_return_requests__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        log.debug(f'Filtering for Credits and Returns')\n",
    "\n",
    "        return data.query(\"doc_category in ['Return', 'Credit Request']\")\n",
    "\n",
    "    @staticmethod\n",
    "    def __clear_nan_values__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        log.debug(f'Clearing NaNs')\n",
    "\n",
    "        return data.replace('nan', '')\n",
    "\n",
    "    @staticmethod\n",
    "    def __create_date_column__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        log.debug(f'Creating date column')\n",
    "\n",
    "        return data.assign(date=lambda x: x.created_on.dt.date) \\\n",
    "            .drop(columns=['created_on'])\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_required_columns__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        log.debug(f'Setting column order')\n",
    "\n",
    "        return data.loc[:, ['sales_doc', 'date', 'created_by', 'doc_category', 'sd_type', 'order_reason',\n",
    "                            'sales_org', 'reference', 'country', 'location', 'region']]\n",
    "\n",
    "    def __load_salesforce_data__(self) -> pd.DataFrame:\n",
    "        log.debug(f'Loading Salesforce data')\n",
    "\n",
    "        return pd.read_parquet(os.path.join(self.data_location, 'SalesForce Credit Codes.parquet'))\n",
    "\n",
    "    @staticmethod\n",
    "    def __set_sf_column_order__(data: pd.DataFrame) -> pd.DataFrame:\n",
    "        log.debug(f'Setting column order')\n",
    "\n",
    "        return data.loc[:, ['case_number', 'sf_credit_code', 'claim_accepted', 'dunning_reason']]\n",
    "\n",
    "    def __copy_sf_data__(self):\n",
    "        log.info(f'Copying SF data')\n",
    "\n",
    "        self.sf_data_file_data_sheet.Range(f\"A2:D{self.sf_data_file_data_sheet.UsedRange.Rows.Count}\") \\\n",
    "            .Copy(Destination=self.output_file_sf_data_sheet.Range(f\"B2\"))\n",
    "\n",
    "    def __copy_sap_data__(self):\n",
    "        log.info(f'Copying SAP data')\n",
    "\n",
    "        self.sap_data_file_data_sheet.Range(f\"A2:K{self.sap_data_file_data_sheet.UsedRange.Rows.Count}\") \\\n",
    "            .Copy(\n",
    "            Destination=self.output_file_sap_data_sheet.Range(\n",
    "                f\"A{self.output_file_sap_data_sheet.UsedRange.Rows.Count + 1}\"))\n",
    "\n",
    "    def __copy_sf_formulas_down__(self):\n",
    "        log.debug(f'Copying SF formulas down')\n",
    "\n",
    "        self.output_file_sf_data_sheet.Range(f\"A2:A{self.output_file_sf_data_sheet.UsedRange.Rows.Count}\").FillDown()\n",
    "        self.output_file_sf_data_sheet.Range(f\"F2:F{self.output_file_sf_data_sheet.UsedRange.Rows.Count}\").FillDown()\n",
    "\n",
    "    def __copy_sap_formulas_down__(self):\n",
    "        log.debug(f'Copying SAP formulas down')\n",
    "\n",
    "        self.output_file_sap_data_sheet.Range(f\"L2:R{self.output_file_sap_data_sheet.UsedRange.Rows.Count}\").FillDown()\n",
    "\n",
    "    def __update_sf_named_range__(self):\n",
    "        log.debug(f'Updating SF Named Range')\n",
    "\n",
    "        used_range = self.output_file_sf_data_sheet.UsedRange.Rows.Count\n",
    "        self.output_file.Names('sf_data').Name = 'sf_data'\n",
    "        self.output_file.Names('sf_data').RefersTo = f\"='SF Data'!$A$1:$F${used_range}\"\n",
    "        self.output_file.Names('sf_data').Comment = ''\n",
    "\n",
    "    def __update_sap_named_range__(self):\n",
    "        log.debug(f'Updating SAP Named Range')\n",
    "\n",
    "        used_range = self.output_file_sap_data_sheet.UsedRange.Rows.Count\n",
    "        self.output_file.Names('sap_data').Name = 'sap_data'\n",
    "        self.output_file.Names('sap_data').RefersTo = f\"='SAP Data'!$A$1:$R${used_range}\"\n",
    "        self.output_file.Names('sap_data').Comment = ''\n",
    "\n",
    "\n",
    "class EuropeanManufacturingLogisticsReport(MonthEndSharedData):\n",
    "    \"\"\"Class to create the monthly report with Logistics and Manufacturing credits for Europe\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.report_name = f'{self.report_prefix}Logistics, Manufacturing credits for Europe'\n",
    "        self.prior_report_name = f'{self.prior_report_prefix}Logistics, Manufacturing credits for Europe'\n",
    "\n",
    "        self.credit_data = pd.DataFrame()\n",
    "        self.request_data = pd.DataFrame()\n",
    "        self.report_data = pd.DataFrame()\n",
    "        self.link_data = pd.DataFrame()\n",
    "\n",
    "        self.xl = None\n",
    "        self.output_file = None\n",
    "        self.output_file_data_sheet = None\n",
    "\n",
    "        self.data_file = None\n",
    "        self.data_file_data_sheet = None\n",
    "\n",
    "    def create_all_monthly_files(self):\n",
    "        \"\"\"Creates monthly files for Five9 disposition reporting\"\"\"\n",
    "        log.info(f'Creating {self.report_name}')\n",
    "\n",
    "        self.process_monthly_data()\n",
    "        self.create_monthly_data_files()\n",
    "\n",
    "    def process_monthly_data(self):\n",
    "        \"\"\"Processes the monthly data ready for outputs to be created\"\"\"\n",
    "        log.info(f'Processing data')\n",
    "\n",
    "        self._get_credit_data_()\n",
    "        self._get_credit_request_data_()\n",
    "        self._merge_datasets_()\n",
    "        self._add_date_()\n",
    "        self._rename_columns_()\n",
    "        self._add_credit_details_()\n",
    "        self._filter_credit_departments_()\n",
    "        self._add_credit_count_()\n",
    "        self._get_columns_()\n",
    "        self._save_data_file_()\n",
    "\n",
    "    def create_monthly_data_files(self):\n",
    "        \"\"\"Creates monthly data files\"\"\"\n",
    "\n",
    "        self._start_excel_()\n",
    "        self._copy_prior_month_file_()\n",
    "        self._open_output_file()\n",
    "        self._open_data_file_()\n",
    "        self._copy_data_into_output_file_()\n",
    "        self._close_data_file_()\n",
    "        self._update_named_range_()\n",
    "        self._update_pivots_()\n",
    "        self._save_and_close_output_file_()\n",
    "        self._quit_excel_()\n",
    "\n",
    "    def _get_credit_data_(self):\n",
    "        log.info(f'Getting credit data')\n",
    "\n",
    "        self.credit_data = CreditHeaderData().get_monthly_data(self.year, self.month) \\\n",
    "                               .query(\"system == 'LI3'\") \\\n",
    "                               .loc[:, ['billing_doc', 'created_on', 'country', 'net_value_usd']]\n",
    "\n",
    "    def _get_credit_request_data_(self):\n",
    "        log.info(f'Getting credit request data')\n",
    "\n",
    "        self.__get_links_from_invoice_to_request__()\n",
    "        self.__get_linked_requests__()\n",
    "\n",
    "    def _merge_datasets_(self):\n",
    "        log.info(f'Merging data')\n",
    "\n",
    "        self.report_data = self.credit_data.merge(self.request_data, on='billing_doc', how='left')\n",
    "\n",
    "    def _add_date_(self):\n",
    "        log.info(f'Adding Date column')\n",
    "\n",
    "        self.report_data = self.report_data.assign(created_on=lambda x: x.created_on.dt.date)\n",
    "\n",
    "    def _rename_columns_(self):\n",
    "        log.info(f'Renaming columns')\n",
    "\n",
    "        self.report_data = self.report_data.rename(\n",
    "            columns={'billing_doc': 'credit_number', 'sales_doc': 'credit_request', 'order_reason': 'credit_code'}\n",
    "        )\n",
    "\n",
    "    def _add_credit_details_(self):\n",
    "        log.info(f'Adding credit code details')\n",
    "\n",
    "        self.report_data = self.report_data.merge(BusinessLogic.credit_codes())\n",
    "\n",
    "    def _filter_credit_departments_(self):\n",
    "        self.report_data = self.report_data \\\n",
    "            .query(\"department in ['Manufacturing', 'Logistics']\")\n",
    "\n",
    "    def _add_credit_count_(self):\n",
    "        log.info(f'Adding credit count column')\n",
    "\n",
    "        self.report_data = self.report_data.assign(credit=1)\n",
    "\n",
    "    def _get_columns_(self):\n",
    "        log.info(f'Setting report column order')\n",
    "\n",
    "        self.report_data = self.report_data.loc[:, ['created_on', 'credit_code', 'country', 'credit_request',\n",
    "                                                    'department', 'code_description', 'net_value_usd', 'credit',\n",
    "                                                    'doc_category', 'sd_type']]\n",
    "\n",
    "    def _save_data_file_(self):\n",
    "        output_file = os.path.join(Locations.temp_dir, f'{self.report_name}.csv')\n",
    "        self.report_data.to_csv(path_or_buf=output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    def _start_excel_(self):\n",
    "        log.info(f'Starting Excel')\n",
    "\n",
    "        self.xl = win32.Dispatch(\"Excel.Application\")\n",
    "        if Locations.environment == \"dev\":\n",
    "            self.xl.DisplayAlerts = True\n",
    "            self.xl.Visible = True\n",
    "        else:\n",
    "            self.xl.DisplayAlerts = False\n",
    "            self.xl.Visible = False\n",
    "\n",
    "    def _copy_prior_month_file_(self):\n",
    "        log.info(f'Copying prior month file')\n",
    "\n",
    "        previous_file_path = os.path.join(self.prior_save_location, f'{self.prior_report_name}.xlsx')\n",
    "        current_file_path = os.path.join(self.save_location, f'{self.report_name}.xlsx')\n",
    "        copyfile(previous_file_path, current_file_path)\n",
    "\n",
    "    def _open_output_file(self):\n",
    "        log.info(f'Opening output file')\n",
    "\n",
    "        self.output_file = self.xl.Workbooks.Open(os.path.join(self.save_location, f'{self.report_name}.xlsx'))\n",
    "        self.output_file_data_sheet = self.output_file.Sheets('credit_data')\n",
    "        self.xl.Application.Calculation = -4135  # to set xlCalculationManual\n",
    "\n",
    "    def _open_data_file_(self):\n",
    "        log.info(f'Opening SAP data file')\n",
    "\n",
    "        self.data_file = self.xl.Workbooks. \\\n",
    "            Open(os.path.join(os.path.join(Locations.temp_dir, self.report_name + '.csv')))\n",
    "        self.data_file_data_sheet = self.data_file.Sheets(f'{self.report_prefix}Logistics, Manufactur')\n",
    "\n",
    "    def _copy_data_into_output_file_(self):\n",
    "        log.info(f'Copying data')\n",
    "\n",
    "        self.data_file_data_sheet.Range(f\"A2:J{self.data_file_data_sheet.UsedRange.Rows.Count}\") \\\n",
    "            .Copy(Destination=self.output_file_data_sheet.Range(f\"A{self.output_file_data_sheet.UsedRange.Rows.Count}\"))\n",
    "\n",
    "    def _close_data_file_(self):\n",
    "        log.info(f'Closing data files')\n",
    "\n",
    "        self.data_file.Close(False)\n",
    "\n",
    "    def _update_named_range_(self):\n",
    "        log.debug(f'Updating Named Range')\n",
    "\n",
    "        used_range = self.output_file_data_sheet.UsedRange.Rows.Count\n",
    "        self.output_file.Names('credit_data').Name = 'credit_data'\n",
    "        self.output_file.Names('credit_data').RefersTo = f\"='credit_data'!$A$1:$J${used_range}\"\n",
    "        self.output_file.Names('credit_data').Comment = ''\n",
    "\n",
    "    def _update_pivots_(self):\n",
    "        log.debug('Updating pivots')\n",
    "\n",
    "        self.xl.Application.Calculation = -4105  # to set xlCalculationAutomatic\n",
    "        self.output_file.RefreshAll()\n",
    "\n",
    "    def _save_and_close_output_file_(self):\n",
    "        log.debug('Saving and closing files')\n",
    "\n",
    "        self.output_file.Sheets(\"Value Heatmap\").Select()\n",
    "        self.output_file.Close(True)\n",
    "\n",
    "    def _quit_excel_(self):\n",
    "        log.debug('Closing Excel')\n",
    "\n",
    "        self.xl.Quit()\n",
    "        del self.xl\n",
    "\n",
    "    def __get_links_from_invoice_to_request__(self):\n",
    "        log.debug(f'Extracting VBFA links for {self.report_name}')\n",
    "\n",
    "        linked_doc_file = os.path.join(Locations.temp_dir, f'{self.report_name} - VBFA')\n",
    "        doc_numbers = self.credit_data.billing_doc.tolist()\n",
    "\n",
    "        if SaveBusinessData.recreate_data_file(f'{linked_doc_file}.csv'):\n",
    "            criteria = {'Follow-on doc.': doc_numbers,\n",
    "                        'Prec.doc.categ.': ['H', 'K']}\n",
    "            required_fields = ['Preceding Doc.', 'Prec.doc.categ.', 'Follow-on doc.', 'Subs.doc.categ.']\n",
    "            ZS16(system_name='LI3') \\\n",
    "                .execute_zs16_report(table_name='VBFA',\n",
    "                                     save_location=Locations.temp_dir,\n",
    "                                     filename=f'{self.report_name} - VBFA.txt',\n",
    "                                     output_fields=required_fields,\n",
    "                                     criteria=copy(criteria))\n",
    "        self.link_data = pd.read_csv(f'{linked_doc_file}.csv', keep_default_na=False, dtype='str') \\\n",
    "            .rename(columns={'follon_doc': 'billing_doc',\n",
    "                             'predecessr': 'sales_doc'}) \\\n",
    "            .drop_duplicates()\n",
    "\n",
    "    def __get_linked_requests__(self):\n",
    "        log.debug(f'Getting linked credit requests')\n",
    "\n",
    "        current_month = (self.year, self.month)\n",
    "        prior_month_date = datetime.date(self.year, self.month, 1) - datetime.timedelta(days=1)\n",
    "        prior_month_2_date = (datetime.date(self.year, self.month, 1) - datetime.timedelta(days=1)) \\\n",
    "                                 .replace(day=1) - datetime.timedelta(days=1)\n",
    "\n",
    "        prior_three_months = [current_month,\n",
    "                              (prior_month_date.year, prior_month_date.month),\n",
    "                              (prior_month_2_date.year, prior_month_2_date.month)]\n",
    "\n",
    "        request_data_dfs = []\n",
    "        for year, month in prior_three_months:\n",
    "            request_data_dfs.append(\n",
    "                OrderHeaderData().get_monthly_data(year, month)\n",
    "                .query(\"doc_category in ['Credit Request', 'Return']\")\n",
    "                .loc[:, ['sales_doc', 'order_reason', 'doc_category', 'sd_type']]\n",
    "            )\n",
    "\n",
    "        self.request_data = pd.concat(request_data_dfs) \\\n",
    "            .merge(self.link_data.loc[:, ['billing_doc', 'sales_doc']], how='right')\n",
    "\n",
    "\n",
    "class MonthlyCSVolumesReport(MonthEndSharedData, BigQueryEmailMeter):\n",
    "    \"\"\"Creates monthly file on CS volumes\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.report_name = f'{self.report_prefix}CS Volumes'\n",
    "\n",
    "        self.index_cols = ['date', 'region', 'location', 'country']\n",
    "\n",
    "        self.date_range = pd.DataFrame()\n",
    "        self.output_data = pd.DataFrame()\n",
    "\n",
    "    def create_all_monthly_files(self) -> None:\n",
    "        \"\"\"Loads all data, pivots everything by date/region/location/country, and merges into a single csv file.\"\"\"\n",
    "        log.info(f'Running {self.report_name}')\n",
    "\n",
    "        self._create_date_range_df_()\n",
    "        self._add_country_data_()\n",
    "        self._get_call_data_()\n",
    "        self._get_order_data_()\n",
    "        self._get_return_data_()\n",
    "        self._get_email_data_()\n",
    "        self._save_file_()\n",
    "\n",
    "    def _create_date_range_df_(self):\n",
    "        log.info(f'Building date range')\n",
    "\n",
    "        self.date_range = (pd.DataFrame(\n",
    "            columns=['date_time'],\n",
    "            data=pd.date_range(start=datetime.date(self.year, self.month, 1),\n",
    "                               end=datetime.date(self.year, self.month, calendar.monthrange(self.year, self.month)[1])\n",
    "                                   + datetime.timedelta(days=1),\n",
    "                               inclusive='left'))\n",
    "                           .assign(date=lambda x: x.date_time.dt.date)\n",
    "                           .drop(columns=['date_time'])\n",
    "                           )\n",
    "\n",
    "    def _add_country_data_(self):\n",
    "        log.info(f'Adding country details')\n",
    "\n",
    "        date_country = []\n",
    "        for date, date_data in self.date_range.groupby('date'):\n",
    "            countries = BusinessLogic.location_hierarchy().loc[:, 'country'].tolist()\n",
    "            dates = [date] * len(countries)\n",
    "            date_countries = [list(x) for x in zip(dates, countries)]\n",
    "\n",
    "            date_country.append(pd.DataFrame(\n",
    "                columns=['date', 'country'],\n",
    "                data=date_countries))\n",
    "\n",
    "        self.output_data = pd.concat(date_country) \\\n",
    "            .merge(BusinessLogic.location_hierarchy(), how='left') \\\n",
    "            .pivot_table(index=self.index_cols, aggfunc=np.sum)\n",
    "\n",
    "    def _get_call_data_(self):\n",
    "        log.info(f'Extracting Call data')\n",
    "\n",
    "        call_data = CallData().get_monthly_data(self.year, self.month) \\\n",
    "                        .assign(date=lambda x: pd.to_datetime(x.call_time).dt.date,\n",
    "                                call=1) \\\n",
    "                        .loc[:, ['date', 'country', 'call']]\n",
    "\n",
    "        self.output_data = self.output_data \\\n",
    "            .join(call_data\n",
    "                  .merge(BusinessLogic.location_hierarchy(), how='left')\n",
    "                  .pivot_table(index=self.index_cols,\n",
    "                               values=['call'],\n",
    "                               aggfunc=np.sum))\n",
    "\n",
    "    def _get_order_data_(self):\n",
    "        log.info(f'Extracting Order data')\n",
    "\n",
    "        order_pivot = OrderHeaderData().get_monthly_data(self.year, self.month) \\\n",
    "            .merge(BusinessLogic.cs_employees_and_system_users()) \\\n",
    "            .merge(BusinessLogic.location_hierarchy()) \\\n",
    "            .query(\"(doc_category in ['C', 'Order'])\") \\\n",
    "            .assign(date=lambda x: x.created_on.dt.date,\n",
    "                    orders=1) \\\n",
    "            .pivot_table(index=self.index_cols,\n",
    "                         values=['orders'],\n",
    "                         aggfunc=np.sum)\n",
    "\n",
    "        self.output_data = self.output_data.join(order_pivot)\n",
    "\n",
    "    def _get_return_data_(self):\n",
    "        log.info(f'Extracting Returns data')\n",
    "\n",
    "        return_pivot = OrderHeaderData().get_monthly_data(self.year, self.month) \\\n",
    "            .query(\"doc_category == 'Return'\") \\\n",
    "            .merge(BusinessLogic.location_hierarchy()) \\\n",
    "            .assign(date=lambda x: x.created_on.dt.date,\n",
    "                    returns=1) \\\n",
    "            .pivot_table(index=self.index_cols,\n",
    "                         values=['returns'],\n",
    "                         aggfunc=np.sum)\n",
    "        self.output_data = self.output_data.join(return_pivot)\n",
    "\n",
    "    def _get_email_data_(self):\n",
    "        log.info(f'Extracting Email data')\n",
    "        london_tz = pytz.timezone('Europe/London')\n",
    "        date_from = datetime.datetime(self.year, self.month, 1, tzinfo=london_tz)\n",
    "        date_to = datetime.datetime(self.year, self.month, calendar.monthrange(self.year, self.month)[1],\n",
    "                                    hour=23, minute=59, second=59, tzinfo=london_tz)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT main_timestamp, gmail_primary_address\n",
    "            FROM {self.destination_table}\n",
    "            WHERE main_timestamp BETWEEN \"{date_from}\" AND \"{date_to}\" \n",
    "                AND action_type = 'Received' \n",
    "                AND is_direct_message = TRUE\n",
    "        \"\"\"\n",
    "\n",
    "        email_data = self.bigquery.query(query).result().to_dataframe() \\\n",
    "            .rename(columns={'gmail_primary_address': 'inbox'}) \\\n",
    "            .merge(BusinessLogic.email_to_country(), how='left', validate='m:1') \\\n",
    "            .merge(BusinessLogic.location_hierarchy(), how='left', validate='m:1') \\\n",
    "            .assign(date=lambda x: x.main_timestamp.dt.date,\n",
    "                    emails=1) \\\n",
    "            .pivot_table(index=self.index_cols,\n",
    "                         values=['emails'],\n",
    "                         aggfunc=np.sum) \\\n",
    "            .fillna(0)\n",
    "        self.output_data = self.output_data.join(email_data)\n",
    "\n",
    "    def _save_file_(self):\n",
    "        filename = os.path.join(self.save_location, f'{self.report_name}.csv')\n",
    "        self.output_data.loc[:, ['orders', 'returns', 'call', 'emails']] \\\n",
    "            .dropna(how='all') \\\n",
    "            .fillna(0) \\\n",
    "            .to_csv(filename, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "class MonthlyOrderTypes(MonthEndSharedData):\n",
    "    \"\"\"Creates output file for Order Entry Types data file\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.report_name = f'{self.report_prefix}Order Entry Types'\n",
    "        self.header_data = pd.DataFrame()\n",
    "        self.line_date = pd.DataFrame()\n",
    "\n",
    "    def create_all_monthly_files(self):\n",
    "        \"\"\"Creates the Order Entry Types file\"\"\"\n",
    "        log.info(f'Running {self.report_name}')\n",
    "\n",
    "        self._get_header_data_()\n",
    "        self._get_line_data_()\n",
    "        self._merge_data_()\n",
    "        self._save_data_()\n",
    "\n",
    "    def _get_header_data_(self):\n",
    "        log.info('Getting header data')\n",
    "\n",
    "        self.header_data = OrderHeaderData().get_monthly_data(self.year, self.month) \\\n",
    "                               .loc[:, ['created_on', 'sales_doc', 'country', 'po_type',\n",
    "                                        'sd_type', 'system', 'sales_org', 'created_by']] \\\n",
    "            .merge(BusinessLogic.cs_location_hierarchy(), how='left') \\\n",
    "            .merge(BusinessLogic.cs_employees_and_system_users())\n",
    "        self.header_data['type'] = self.header_data.apply(Utilities().get_order_type, axis='columns')\n",
    "\n",
    "    def _get_line_data_(self):\n",
    "        log.info('Getting line data')\n",
    "\n",
    "        line_data = OrderLineData().get_monthly_data(self.year, self.month) \\\n",
    "                        .loc[:, ['sales_doc']] \\\n",
    "            .assign(lines=1)\n",
    "        self.line_date = line_data.pivot_table(index='sales_doc', values='lines', aggfunc=np.sum)\n",
    "\n",
    "    def _merge_data_(self):\n",
    "        log.info('Mergine header and line data')\n",
    "\n",
    "        self.header_data = self.header_data \\\n",
    "            .merge(self.line_date, left_on='sales_doc', right_index=True, how='left') \\\n",
    "            .assign(year=self.year,\n",
    "                    month=self.month) \\\n",
    "            .pivot_table(index=['region', 'country', 'type', 'year', 'month'],\n",
    "                         values=['lines', 'sales_doc'],\n",
    "                         aggfunc={'lines': np.sum, 'sales_doc': lambda x: len(x.unique())}) \\\n",
    "            .rename(columns={'sales_doc': 'orders'})\n",
    "\n",
    "    def _save_data_(self):\n",
    "        log.info('Saving data')\n",
    "\n",
    "        self.header_data.to_csv(os.path.join(self.save_location, f'{self.report_name}.csv'), encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "class MonthlyEmailReport(MonthEndSharedData, MonthEndReporting, BigQueryEmailMeter):\n",
    "    \"\"\"Creates the monthly email volumes data files from Email Meter\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.report_name = f'{self.report_prefix}Global Emails'\n",
    "\n",
    "        london_tz = pytz.timezone('Europe/London')\n",
    "        self.date_from = datetime.datetime(self.year, self.month, 1, tzinfo=london_tz)\n",
    "        self.date_to = datetime.datetime(self.year, self.month, calendar.monthrange(self.year, self.month)[1],\n",
    "                                         hour=23, minute=59, second=59, tzinfo=london_tz)\n",
    "\n",
    "    def create_all_monthly_files(self):\n",
    "        \"\"\"Creates all monthly files for Monthly Email Report\"\"\"\n",
    "        self.process_monthly_data()\n",
    "        self.create_monthly_data_files()\n",
    "        self.create_monthly_qv_files()\n",
    "\n",
    "    def process_monthly_data(self):\n",
    "        \"\"\"Processes monthly email data\"\"\"\n",
    "        query = f\"\"\"\n",
    "            SELECT main_timestamp, gmail_primary_address, action_type, subject, from_original_address, recipients_to, \n",
    "            recipients_cc, recipients_bcc, is_direct_message, is_first_message, is_intradomain, is_replied, reply_data, \n",
    "            is_first_reply  \n",
    "            FROM {self.destination_table}\n",
    "            WHERE main_timestamp BETWEEN \"{self.date_from}\" AND \"{self.date_to}\" \n",
    "                AND is_spam = FALSE \n",
    "                AND is_automated_email = \"Not Automated\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.bigquery.query(query).result().to_dataframe() \\\n",
    "            .assign(working_response_time=lambda x: x.reply_data.str[0].str['reply_time'],\n",
    "                    replied_by=lambda x: x.reply_data.str[0].str['replied_by']) \\\n",
    "            .drop(columns=['reply_data']) \\\n",
    "            .to_csv(os.path.join(Locations.email_data_dir, f'{self.report_name}.csv'),\n",
    "                    index=False, encoding='utf-8-sig')\n",
    "\n",
    "    def create_monthly_data_files(self):\n",
    "        \"\"\"Creates data files\"\"\"\n",
    "        EmailMeter().import_email_meter_data()\n",
    "\n",
    "    def create_monthly_qv_files(self):\n",
    "        \"\"\"Creates QlikView source files\"\"\"\n",
    "        log.info(f'Creating QlikView file')\n",
    "\n",
    "        sql_query = f\"\"\"\n",
    "            SELECT country, reply_data, main_timestamp\n",
    "            FROM {self.destination_table}\n",
    "            WHERE main_timestamp BETWEEN \"{self.date_from}\" AND \"{self.date_to}\" \n",
    "                AND is_spam = FALSE \n",
    "                AND is_automated_email = \"Not Automated\"\n",
    "                AND is_replied = TRUE\n",
    "            \"\"\"\n",
    "\n",
    "        qv_file = os.path.join(self.qv_file_location, f'{self.report_prefix}Emails.csv')\n",
    "        self.bigquery.query(sql_query).result().to_dataframe() \\\n",
    "            .assign(working_response_time=lambda x: x.reply_data.str[0].str['reply_time'],\n",
    "                    reporting_month=lambda x: datetime.date(self.year, self.month, 1).strftime('%d/%m/%Y'),\n",
    "                    BU=lambda x: 'Diversey Care') \\\n",
    "            .drop(columns=['reply_data', 'main_timestamp']) \\\n",
    "            .replace({'United Kingdom (Zenith)': 'United Kingdom (Direct)'}) \\\n",
    "            .merge(BusinessLogic.cs_location_hierarchy().loc[:, ['country', 'location']]) \\\n",
    "            .to_csv(qv_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "class MonthlyFive9EmailReport(MonthEndSharedData):\n",
    "    \"\"\"Extracts and saves the monthly Five9 email report for the US domain\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.report_name = f'{self.report_prefix}Emails - Five9'\n",
    "\n",
    "        report_date = datetime.date(self.year, self.month + 1, 1)\n",
    "        day_start = datetime.time(tzinfo=pytz.UTC)\n",
    "        day_end = datetime.time(hour=23, minute=59, second=59, tzinfo=pytz.UTC)\n",
    "\n",
    "        self.email_after = int(datetime.datetime.combine(report_date, day_start).timestamp())\n",
    "        self.email_before = int(datetime.datetime.combine(report_date, day_end).timestamp())\n",
    "\n",
    "    def create_all_monthly_files(self):\n",
    "        \"\"\"Downloads the Five9 US Domain email report, and creates parquet file\"\"\"\n",
    "        log.info(f'Creating {self.report_name}')\n",
    "\n",
    "        self._download_report_()\n",
    "        self._create_parquet_file_()\n",
    "        self._create_qv_file_()\n",
    "\n",
    "    def _download_report_(self):\n",
    "        log.info(f'Downloading emailed report')\n",
    "\n",
    "        data_file = os.path.join(Locations.email_data_dir, f'{self.report_name}.csv')\n",
    "        print(data_file)\n",
    "        if SaveBusinessData.recreate_data_file(data_file):\n",
    "            search_string = f'from:reports-noreply@five9.com ' \\\n",
    "                            f'subject:\"Scheduled Report: Monthly US Email Response Times\" ' \\\n",
    "                            f'after:{self.email_after} ' \\\n",
    "                            f'before:{self.email_before}'\n",
    "            \n",
    "            print(search_string)\n",
    "\n",
    "            Utilities().download_email_attachment(file_name=data_file,\n",
    "                                                  search_string=search_string)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_parquet_file_():\n",
    "        log.info('Creating parquet file')\n",
    "\n",
    "        EmailMeter().import_five9_emails()\n",
    "\n",
    "    def _create_qv_file_(self):\n",
    "        five9_monthly_file = os.path.join(Locations.email_data_dir, f'{self.report_name}.parquet')\n",
    "        email_cols = {'date': 'datetime64[ns]', 'action_type': str, 'country': str, 'is_replied': bool,\n",
    "                      'working_response_time': int, 'location': str}\n",
    "\n",
    "        pd.read_parquet(five9_monthly_file) \\\n",
    "            .merge(BusinessLogic.email_to_country(), how='left') \\\n",
    "            .merge(BusinessLogic.cs_location_hierarchy()) \\\n",
    "            .query(\"(is_replied == True) and (working_response_time >= 0)\") \\\n",
    "            .loc[:, list(email_cols.keys())] \\\n",
    "            .assign(BU=lambda x: np.where(x.location == 'Welham Green', 'Diversey Care', 'Diversey Care'),\n",
    "                    reporting_month=f'01/{str(self.month).zfill(2)}/{self.year}') \\\n",
    "            .drop(columns=['date', 'action_type', 'is_replied']) \\\n",
    "            .to_csv(os.path.join(self.qv_file_location, f'{self.report_name}.csv'), encoding='utf-8-sig', index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Logging('development.log', log_level='INFO')\n",
    "\n",
    "    MonthlyReporting().run_month_end_reports()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdee7c6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['1'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_46060/3119692408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\A803377\\Downloads\\Outbound freight - file to split.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5266\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5267\u001b[0m         \"\"\"\n\u001b[1;32m-> 5268\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   5269\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5270\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4548\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4549\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4551\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4590\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4591\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6695\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6696\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6697\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6698\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['1'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\A803377\\Downloads\\Outbound freight - file to split.xlsx')\n",
    "df.drop(columns=['1'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac49b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Outbound_freight__1.xlsx\n",
      "Saved: Outbound_freight__2.xlsx\n",
      "Saved: Outbound_freight__3.xlsx\n",
      "Saved: Outbound_freight__4.xlsx\n",
      "Saved: Outbound_freight__5.xlsx\n",
      "Saved: Outbound_freight__6.xlsx\n",
      "Saved: Outbound_freight__7.xlsx\n",
      "Saved: Outbound_freight__8.xlsx\n",
      "Saved: Outbound_freight__9.xlsx\n",
      "Saved: Outbound_freight__10.xlsx\n",
      "Saved: Outbound_freight__11.xlsx\n",
      "Saved: Outbound_freight__12.xlsx\n",
      "Saved: Outbound_freight__13.xlsx\n",
      "Saved: Outbound_freight__14.xlsx\n",
      "Saved: Outbound_freight__15.xlsx\n",
      "Saved: Outbound_freight__16.xlsx\n",
      "Saved: Outbound_freight__17.xlsx\n",
      "Saved: Outbound_freight__18.xlsx\n",
      "Saved: Outbound_freight__19.xlsx\n",
      "Saved: Outbound_freight__20.xlsx\n",
      "Saved: Outbound_freight__21.xlsx\n",
      "Saved: Outbound_freight__22.xlsx\n",
      "Saved: Outbound_freight__23.xlsx\n",
      "Saved: Outbound_freight__24.xlsx\n",
      "Saved: Outbound_freight__25.xlsx\n",
      "Saved: Outbound_freight__26.xlsx\n",
      "Saved: Outbound_freight__27.xlsx\n",
      "Saved: Outbound_freight__28.xlsx\n",
      "Saved: Outbound_freight__29.xlsx\n",
      "Saved: Outbound_freight__30.xlsx\n",
      "Saved: Outbound_freight__31.xlsx\n",
      "Saved: Outbound_freight__32.xlsx\n",
      "Saved: Outbound_freight__33.xlsx\n",
      "Saved: Outbound_freight__34.xlsx\n",
      "Saved: Outbound_freight__35.xlsx\n",
      "Saved: Outbound_freight__36.xlsx\n",
      "Saved: Outbound_freight__37.xlsx\n",
      "Saved: Outbound_freight__38.xlsx\n",
      "Saved: Outbound_freight__39.xlsx\n",
      "Saved: Outbound_freight__40.xlsx\n",
      "Saved: Outbound_freight__41.xlsx\n",
      "Saved: Outbound_freight__42.xlsx\n",
      "Saved: Outbound_freight__43.xlsx\n",
      "Saved: Outbound_freight__44.xlsx\n",
      "Saved: Outbound_freight__45.xlsx\n",
      "Saved: Outbound_freight__46.xlsx\n",
      "Saved: Outbound_freight__47.xlsx\n",
      "Saved: Outbound_freight__48.xlsx\n",
      "Saved: Outbound_freight__49.xlsx\n",
      "Saved: Outbound_freight__50.xlsx\n",
      "Saved: Outbound_freight__51.xlsx\n",
      "Saved: Outbound_freight__52.xlsx\n",
      "Saved: Outbound_freight__53.xlsx\n",
      "Saved: Outbound_freight__54.xlsx\n",
      "Saved: Outbound_freight__55.xlsx\n",
      "Saved: Outbound_freight__56.xlsx\n",
      "Saved: Outbound_freight__57.xlsx\n",
      "Saved: Outbound_freight__58.xlsx\n",
      "Saved: Outbound_freight__59.xlsx\n",
      "Saved: Outbound_freight__60.xlsx\n",
      "Saved: Outbound_freight__61.xlsx\n",
      "Saved: Outbound_freight__62.xlsx\n",
      "Saved: Outbound_freight__63.xlsx\n",
      "Saved: Outbound_freight__64.xlsx\n",
      "Saved: Outbound_freight__65.xlsx\n",
      "Saved: Outbound_freight__66.xlsx\n",
      "Saved: Outbound_freight__67.xlsx\n",
      "Saved: Outbound_freight__68.xlsx\n",
      "Saved: Outbound_freight__69.xlsx\n",
      "Saved: Outbound_freight__70.xlsx\n",
      "Saved: Outbound_freight__71.xlsx\n",
      "Saved: Outbound_freight__72.xlsx\n",
      "Saved: Outbound_freight__73.xlsx\n",
      "Saved: Outbound_freight__74.xlsx\n",
      "Saved: Outbound_freight__75.xlsx\n",
      "Saved: Outbound_freight__76.xlsx\n",
      "Saved: Outbound_freight__77.xlsx\n",
      "Saved: Outbound_freight__78.xlsx\n",
      "Saved: Outbound_freight__79.xlsx\n",
      "Saved: Outbound_freight__80.xlsx\n",
      "Saved: Outbound_freight__81.xlsx\n",
      "Saved: Outbound_freight__82.xlsx\n",
      "Saved: Outbound_freight__83.xlsx\n",
      "Saved: Outbound_freight__84.xlsx\n",
      "Saved: Outbound_freight__85.xlsx\n",
      "Saved: Outbound_freight__86.xlsx\n",
      "Saved: Outbound_freight__87.xlsx\n",
      "Saved: Outbound_freight__88.xlsx\n",
      "Saved: Outbound_freight__89.xlsx\n",
      "Saved: Outbound_freight__90.xlsx\n",
      "Saved: Outbound_freight__91.xlsx\n",
      "Saved: Outbound_freight__92.xlsx\n",
      "Saved: Outbound_freight__93.xlsx\n",
      "Saved: Outbound_freight__94.xlsx\n",
      "Saved: Outbound_freight__95.xlsx\n",
      "Saved: Outbound_freight__96.xlsx\n",
      "Saved: Outbound_freight__97.xlsx\n",
      "Saved: Outbound_freight__98.xlsx\n",
      "Saved: Outbound_freight__99.xlsx\n",
      "Saved: Outbound_freight__100.xlsx\n",
      "Saved: Outbound_freight__101.xlsx\n",
      "Saved: Outbound_freight__102.xlsx\n",
      "Saved: Outbound_freight__103.xlsx\n",
      "Saved: Outbound_freight__104.xlsx\n",
      "Saved: Outbound_freight__105.xlsx\n",
      "Saved: Outbound_freight__106.xlsx\n",
      "Saved: Outbound_freight__107.xlsx\n",
      "Saved: Outbound_freight__108.xlsx\n",
      "Saved: Outbound_freight__109.xlsx\n",
      "Saved: Outbound_freight__110.xlsx\n",
      "Saved: Outbound_freight__111.xlsx\n",
      "Saved: Outbound_freight__112.xlsx\n",
      "Saved: Outbound_freight__113.xlsx\n",
      "Saved: Outbound_freight__114.xlsx\n",
      "Saved: Outbound_freight__115.xlsx\n",
      "Saved: Outbound_freight__116.xlsx\n",
      "Saved: Outbound_freight__117.xlsx\n",
      "Saved: Outbound_freight__118.xlsx\n",
      "Saved: Outbound_freight__119.xlsx\n",
      "Saved: Outbound_freight__120.xlsx\n",
      "Saved: Outbound_freight__121.xlsx\n",
      "Saved: Outbound_freight__122.xlsx\n",
      "Saved: Outbound_freight__123.xlsx\n",
      "Saved: Outbound_freight__124.xlsx\n",
      "Saved: Outbound_freight__125.xlsx\n",
      "Saved: Outbound_freight__126.xlsx\n",
      "Saved: Outbound_freight__127.xlsx\n",
      "Saved: Outbound_freight__128.xlsx\n",
      "Saved: Outbound_freight__129.xlsx\n",
      "Saved: Outbound_freight__130.xlsx\n",
      "Saved: Outbound_freight__131.xlsx\n",
      "Saved: Outbound_freight__132.xlsx\n",
      "Saved: Outbound_freight__133.xlsx\n",
      "Saved: Outbound_freight__134.xlsx\n",
      "Saved: Outbound_freight__135.xlsx\n",
      "Saved: Outbound_freight__136.xlsx\n",
      "Saved: Outbound_freight__137.xlsx\n",
      "Saved: Outbound_freight__138.xlsx\n",
      "Saved: Outbound_freight__139.xlsx\n",
      "Saved: Outbound_freight__140.xlsx\n",
      "Saved: Outbound_freight__141.xlsx\n",
      "Saved: Outbound_freight__142.xlsx\n",
      "Saved: Outbound_freight__143.xlsx\n",
      "Saved: Outbound_freight__144.xlsx\n",
      "Saved: Outbound_freight__145.xlsx\n",
      "Saved: Outbound_freight__146.xlsx\n",
      "Saved: Outbound_freight__147.xlsx\n",
      "Saved: Outbound_freight__148.xlsx\n",
      "Saved: Outbound_freight__149.xlsx\n",
      "Saved: Outbound_freight__150.xlsx\n",
      "Saved: Outbound_freight__151.xlsx\n",
      "Saved: Outbound_freight__152.xlsx\n",
      "Saved: Outbound_freight__153.xlsx\n",
      "Saved: Outbound_freight__154.xlsx\n",
      "Saved: Outbound_freight__155.xlsx\n",
      "Saved: Outbound_freight__156.xlsx\n",
      "Saved: Outbound_freight__157.xlsx\n",
      "Saved: Outbound_freight__158.xlsx\n",
      "Saved: Outbound_freight__159.xlsx\n",
      "Saved: Outbound_freight__160.xlsx\n",
      "Saved: Outbound_freight__161.xlsx\n",
      "Saved: Outbound_freight__162.xlsx\n",
      "Saved: Outbound_freight__163.xlsx\n",
      "Saved: Outbound_freight__164.xlsx\n",
      "Saved: Outbound_freight__165.xlsx\n",
      "Saved: Outbound_freight__166.xlsx\n",
      "Saved: Outbound_freight__167.xlsx\n",
      "Saved: Outbound_freight__168.xlsx\n",
      "Saved: Outbound_freight__169.xlsx\n",
      "Saved: Outbound_freight__170.xlsx\n",
      "Saved: Outbound_freight__171.xlsx\n",
      "Saved: Outbound_freight__172.xlsx\n",
      "Saved: Outbound_freight__173.xlsx\n",
      "Saved: Outbound_freight__174.xlsx\n",
      "Saved: Outbound_freight__175.xlsx\n",
      "Saved: Outbound_freight__176.xlsx\n",
      "Saved: Outbound_freight__177.xlsx\n",
      "Saved: Outbound_freight__178.xlsx\n",
      "Saved: Outbound_freight__179.xlsx\n",
      "Saved: Outbound_freight__180.xlsx\n",
      "Saved: Outbound_freight__181.xlsx\n",
      "Saved: Outbound_freight__182.xlsx\n",
      "Saved: Outbound_freight__183.xlsx\n",
      "Saved: Outbound_freight__184.xlsx\n",
      "Saved: Outbound_freight__185.xlsx\n",
      "Saved: Outbound_freight__186.xlsx\n",
      "Saved: Outbound_freight__187.xlsx\n",
      "Saved: Outbound_freight__188.xlsx\n",
      "Saved: Outbound_freight__189.xlsx\n",
      "Saved: Outbound_freight__190.xlsx\n",
      "Saved: Outbound_freight__191.xlsx\n",
      "Saved: Outbound_freight__192.xlsx\n",
      "Saved: Outbound_freight__193.xlsx\n",
      "Saved: Outbound_freight__194.xlsx\n",
      "Saved: Outbound_freight__195.xlsx\n",
      "Saved: Outbound_freight__196.xlsx\n",
      "Saved: Outbound_freight__197.xlsx\n",
      "Saved: Outbound_freight__198.xlsx\n",
      "Saved: Outbound_freight__199.xlsx\n",
      "Saved: Outbound_freight__200.xlsx\n",
      "Saved: Outbound_freight__201.xlsx\n",
      "Saved: Outbound_freight__202.xlsx\n",
      "Saved: Outbound_freight__203.xlsx\n",
      "Saved: Outbound_freight__204.xlsx\n",
      "Saved: Outbound_freight__205.xlsx\n",
      "Saved: Outbound_freight__206.xlsx\n",
      "Saved: Outbound_freight__207.xlsx\n",
      "Saved: Outbound_freight__208.xlsx\n",
      "Saved: Outbound_freight__209.xlsx\n",
      "Saved: Outbound_freight__210.xlsx\n",
      "Saved: Outbound_freight__211.xlsx\n",
      "Saved: Outbound_freight__212.xlsx\n",
      "Saved: Outbound_freight__213.xlsx\n",
      "Saved: Outbound_freight__214.xlsx\n",
      "Saved: Outbound_freight__215.xlsx\n",
      "Saved: Outbound_freight__216.xlsx\n",
      "Saved: Outbound_freight__217.xlsx\n",
      "Saved: Outbound_freight__218.xlsx\n",
      "Saved: Outbound_freight__219.xlsx\n",
      "Saved: Outbound_freight__220.xlsx\n",
      "Saved: Outbound_freight__221.xlsx\n",
      "Saved: Outbound_freight__222.xlsx\n",
      "Saved: Outbound_freight__223.xlsx\n",
      "Saved: Outbound_freight__224.xlsx\n",
      "Saved: Outbound_freight__225.xlsx\n",
      "Saved: Outbound_freight__226.xlsx\n",
      "Saved: Outbound_freight__227.xlsx\n",
      "Saved: Outbound_freight__228.xlsx\n",
      "Saved: Outbound_freight__229.xlsx\n",
      "Saved: Outbound_freight__230.xlsx\n",
      "Saved: Outbound_freight__231.xlsx\n",
      "Saved: Outbound_freight__232.xlsx\n",
      "Saved: Outbound_freight__233.xlsx\n",
      "Saved: Outbound_freight__234.xlsx\n",
      "Saved: Outbound_freight__235.xlsx\n",
      "Saved: Outbound_freight__236.xlsx\n",
      "Saved: Outbound_freight__237.xlsx\n",
      "Saved: Outbound_freight__238.xlsx\n",
      "Saved: Outbound_freight__239.xlsx\n",
      "Saved: Outbound_freight__240.xlsx\n",
      "Saved: Outbound_freight__241.xlsx\n",
      "Saved: Outbound_freight__242.xlsx\n",
      "Saved: Outbound_freight__243.xlsx\n",
      "Saved: Outbound_freight__244.xlsx\n",
      "Saved: Outbound_freight__245.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Outbound_freight__246.xlsx\n",
      "Saved: Outbound_freight__247.xlsx\n",
      "Saved: Outbound_freight__248.xlsx\n",
      "Saved: Outbound_freight__249.xlsx\n",
      "Saved: Outbound_freight__250.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_excel_file(input_file, output_file_prefix, records_per_file):\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "#     df.drop(columns=['1'], inplace=True)\n",
    "\n",
    "    # Calculate how many parts the DataFrame should be split into\n",
    "    num_parts = len(df) // records_per_file + (1 if len(df) % records_per_file > 0 else 0)\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        # Calculate start and end row indices for this part\n",
    "        start_row = i * records_per_file\n",
    "        end_row = start_row + records_per_file\n",
    "\n",
    "        # Create a subset of the DataFrame for the current part\n",
    "        df_part = df.iloc[start_row:end_row]\n",
    "\n",
    "        # Save this part to a new Excel file\n",
    "        output_file = f\"{output_file_prefix}_{i + 1}.xlsx\"\n",
    "        df_part.to_excel(output_file, index=False)\n",
    "\n",
    "        print(f\"Saved: {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = r'C:\\Users\\A803377\\Downloads\\Outbound freight - file to split.xlsx'  # Path to your input Excel file\n",
    "output_file_prefix = 'Outbound_freight_'  # Prefix for the output files\n",
    "records_per_file = 902  # Number of records per output file\n",
    "\n",
    "split_excel_file(input_file, output_file_prefix, records_per_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f3ed1ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_46060/1346794383.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mindicator_column\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Loop ID Column'\u001b[0m  \u001b[1;31m# Column name by which you want to split the file (e.g., 'Department', 'Region', etc.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0msplit_excel_file_by_indicator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_46060/1346794383.py\u001b[0m in \u001b[0;36msplit_excel_file_by_indicator\u001b[1;34m(input_file, output_file_prefix, indicator_column)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplit_excel_file_by_indicator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Load the Excel file into a DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Get unique values from the indicator column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         data = io.parse(\n\u001b[0m\u001b[0;32m    487\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m   1549\u001b[0m             \u001b[0mDataFrame\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mExcel\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m         \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m         return self._reader.parse(\n\u001b[0m\u001b[0;32m   1552\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m             \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mfile_rows_needed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_calc_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_sheet_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_rows_needed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"close\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[1;31m# pyxlsb opens two TemporaryFiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py\u001b[0m in \u001b[0;36mget_sheet_data\u001b[1;34m(self, sheet, file_rows_needed)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[0mlast_row_with_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mconverted_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mconverted_row\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconverted_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                 \u001b[1;31m# trim trailing empty elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[0mlast_row_with_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mconverted_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mconverted_row\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconverted_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                 \u001b[1;31m# trim trailing empty elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py\u001b[0m in \u001b[0;36m_convert_cell\u001b[1;34m(self, cell)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mScalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m         from openpyxl.cell.cell import (\n\u001b[0m\u001b[0;32m    578\u001b[0m             \u001b[0mTYPE_ERROR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m             \u001b[0mTYPE_NUMERIC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_excel_file_by_indicator(input_file, output_file_prefix, indicator_column):\n",
    "    # Load the Excel file into a DataFrame\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # Get unique values from the indicator column\n",
    "    unique_values = df[indicator_column].unique()\n",
    "\n",
    "    # Loop through each unique value in the indicator column\n",
    "    for value in unique_values:\n",
    "        # Filter the DataFrame to only include rows where the indicator column matches the current value\n",
    "        df_filtered = df[df[indicator_column] == value]\n",
    "\n",
    "        # Define the output file name based on the indicator value\n",
    "        output_file = f\"{output_file_prefix}_{value}.xlsx\"\n",
    "\n",
    "        # Save the filtered DataFrame to a new Excel file without the headers\n",
    "        df_filtered.to_excel(output_file, index=False, header=True)\n",
    "\n",
    "        print(f\"Saved: {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = r'C:\\Users\\A803377\\Downloads\\Outbound freight - file to split.xlsx'  # Path to your input Excel file\n",
    "output_file_prefix = 'Outbound freight'  # Prefix for the output files\n",
    "indicator_column = 'Loop ID Column'  # Column name by which you want to split the file (e.g., 'Department', 'Region', etc.)\n",
    "\n",
    "split_excel_file_by_indicator(input_file, output_file_prefix, indicator_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162e476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
